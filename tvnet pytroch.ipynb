{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Package import ####\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TVNet(object):\n",
    "    GRAD_IS_ZERO = 1e-12\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def grey_scale_image(self, x):\n",
    "        imageTransform = torchvision.transforms.ToPILImage()\n",
    "        tensorTransform = torchvision.transforms.ToTensor()\n",
    "        greyScaleTransform = torchvision.transforms.Grayscale(num_output_channels=1)\n",
    "        grey_x = greyScaleTransform(x)\n",
    "        \n",
    "        # return tf.floor(grey_x * 255)\n",
    "        \n",
    "        return torch.floor(tensorTransform(grey_x) * 255)\n",
    "    \n",
    "    def normalize_img_tensor(self, x):\n",
    "        return (x - torch.min(x)) / (torch.max(x) - torch.min(x))\n",
    "\n",
    "    def gaussian_smooth(self, x):\n",
    "        nb_channels = 1\n",
    "        \n",
    "        h, w = x.size()\n",
    "        y = torch.ones(1, nb_channels, h, w)\n",
    "        y[:, :, :, :] = x;\n",
    "        weights = torch.tensor([[0.000874, 0.006976, 0.01386, 0.006976, 0.000874],\n",
    "                        [0.006976, 0.0557, 0.110656, 0.0557, 0.006976],\n",
    "                        [0.01386, 0.110656, 0.219833, 0.110656, 0.01386],\n",
    "                        [0.006976, 0.0557, 0.110656, 0.0557, 0.006976],\n",
    "                        [0.000874, 0.006976, 0.01386, 0.006976, 0.000874]])\n",
    "        weights = weights.view(1, nb_channels, 5, 5).repeat(1, nb_channels, 1, 1)\n",
    "\n",
    "        return torch.squeeze(F.conv2d(y, weights))\n",
    "        \n",
    "    '''\n",
    "    def warp_image(self, x, u, v):\n",
    "        #assert len(x.shape) == 4\n",
    "        #assert len(u.shape) == 3\n",
    "        #assert len(v.shape) == 3\n",
    "\n",
    "        # rescale the unit to be pixel\n",
    "        # u = u / x.shape[2].value * 2\n",
    "        # v = v / x.shape[1].value * 2\n",
    "        u = u / x.size()[2] * 2\n",
    "        v = v / x.size()[1] * 2\n",
    "\n",
    "        # delta = tf.concat(axis=1, values=[u, v])\n",
    "        delta = torch.cat([u, v],dim=1)\n",
    "        # return spatial_transformer.transformer(x, delta, (x.shape[-3].value, x.shape[-2].value))\n",
    "    '''    \n",
    "    \n",
    "    \n",
    "    def centered_gradient(self, img):\n",
    "        nx, ny = img.size()\n",
    "        \n",
    "        dy = torch.zeros(img.size())\n",
    "        \n",
    "        for i in range(1, nx - 1):\n",
    "            dy[i,:] = img[i+1,:] -  img[i-1,:]\n",
    "        dy[0,:] = img[1,:] - img[0,:]\n",
    "        dy[ny - 1,:] = img[ny-1,:] - img[ny-2,:]\n",
    "\n",
    "        dx = torch.zeros(img.size())\n",
    "        \n",
    "        for i in range(1, ny - 1):\n",
    "            dx[:,i] = img[:,i+1] - img[:,i-1]\n",
    "        dx[:,0] = img[:,1] - img[:,0]\n",
    "        dx[:, ny - 1] = img[:, ny - 1] - img[:, ny - 2]\n",
    "\n",
    "        dx = dx * .5\n",
    "        dy = dy * .5\n",
    "\n",
    "        return dx, dy\n",
    "    \n",
    "    def zoom_size(self, height, width, factor):\n",
    "        new_height = int(float(height) * factor + 0.5)\n",
    "        new_width = int(float(width) * factor + 0.5)\n",
    "\n",
    "        return new_height, new_width\n",
    "\n",
    "    def zoom_image(self, x, new_height, new_width):\n",
    "        # assert len(x.shape) == 4\n",
    "        \n",
    "        delta = torch.zeros(x.size()[0], 2, new_height * new_width)\n",
    "        # delta = tf.zeros((tf.shape(x)[0], 2, new_height * new_width))\n",
    "        # zoomed_x = spatial_transformer.transformer(x, delta, (new_height, new_width))\n",
    "        # return tf.reshape(zoomed_x, [tf.shape(x)[0], new_height, new_width, x.shape[-1].value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256)\n",
      "tensor([[0.0451, 0.0451, 0.0533,  ..., 0.0656, 0.0574, 0.0574],\n",
      "        [0.0492, 0.0451, 0.0205,  ..., 0.0574, 0.0533, 0.0574],\n",
      "        [0.0533, 0.0533, 0.0533,  ..., 0.0533, 0.0533, 0.0574],\n",
      "        ...,\n",
      "        [0.5205, 0.5369, 0.5369,  ..., 0.0287, 0.0287, 0.0492],\n",
      "        [0.5164, 0.5369, 0.5164,  ..., 0.0287, 0.0328, 0.0369],\n",
      "        [0.5164, 0.5451, 0.5369,  ..., 0.0369, 0.5164, 0.5451]])\n",
      "torch.Size([256, 256])\n",
      "tensor([[0.0448, 0.0488, 0.0523,  ..., 0.0569, 0.0567, 0.0555],\n",
      "        [0.0427, 0.0475, 0.0522,  ..., 0.0576, 0.0576, 0.0555],\n",
      "        [0.0417, 0.0466, 0.0517,  ..., 0.0575, 0.0575, 0.0544],\n",
      "        ...,\n",
      "        [0.5514, 0.5647, 0.5776,  ..., 0.0398, 0.0418, 0.0466],\n",
      "        [0.5388, 0.5526, 0.5648,  ..., 0.0341, 0.0349, 0.0374],\n",
      "        [0.5350, 0.5454, 0.5531,  ..., 0.0312, 0.0313, 0.0353]])\n",
      "torch.Size([252, 252])\n",
      "tensor([[ 0.0020,  0.0038,  0.0024,  ...,  0.0001, -0.0007, -0.0006],\n",
      "        [ 0.0024,  0.0047,  0.0030,  ...,  0.0004, -0.0011, -0.0010],\n",
      "        [ 0.0024,  0.0050,  0.0034,  ...,  0.0007, -0.0015, -0.0016],\n",
      "        ...,\n",
      "        [ 0.0067,  0.0131,  0.0082,  ...,  0.0021,  0.0034,  0.0024],\n",
      "        [ 0.0069,  0.0130,  0.0091,  ...,  0.0011,  0.0017,  0.0013],\n",
      "        [ 0.0052,  0.0090,  0.0069,  ...,  0.0004,  0.0021,  0.0020]])\n",
      "tensor([[-0.0011, -0.0007, -0.0001,  ...,  0.0004,  0.0004, -0.0000],\n",
      "        [-0.0015, -0.0011, -0.0003,  ...,  0.0003,  0.0004, -0.0006],\n",
      "        [-0.0006, -0.0005, -0.0004,  ..., -0.0013, -0.0015, -0.0017],\n",
      "        ...,\n",
      "        [-0.0095, -0.0093, -0.0096,  ..., -0.0065, -0.0073, -0.0080],\n",
      "        [-0.0082, -0.0097, -0.0123,  ..., -0.0043, -0.0052, -0.0057],\n",
      "        [-0.0019, -0.0036, -0.0059,  ..., -0.0014, -0.0018, -0.0010]])\n"
     ]
    }
   ],
   "source": [
    "# load image\n",
    "img1 = Image.open(\"/home/truppr/sandbox/tvnet/testFrames/toys1.png\")\n",
    "print(img1.size)\n",
    "\n",
    "# start TVNet model\n",
    "T = TVNet()\n",
    "\n",
    "# convert to greyscale and normalize image\n",
    "img1 = torch.squeeze(T.normalize_img_tensor(T.grey_scale_image(img1)))\n",
    "print(img1)\n",
    "print(img1.size())\n",
    "\n",
    "# apply gaussian smoothing\n",
    "img1 = T.gaussian_smooth(img1)\n",
    "print(img1)\n",
    "print(img1.size())\n",
    "\n",
    "#u_flat = tf.reshape(u1, (tf.shape(x2)[0], 1, x2.shape[1].value * x2.shape[2].value))\n",
    "#v_flat = tf.reshape(u2, (tf.shape(x2)[0], 1, x2.shape[1].value * x2.shape[2].value))\n",
    "# warped = T.warp_image(img1,u_flat,v_flat)\n",
    "\n",
    "# computer centered gradient\n",
    "img1_dx, img1_dy = T.centered_gradient(img1)\n",
    "print(img1_dx)\n",
    "print(img1_dy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
